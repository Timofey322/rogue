#!/usr/bin/env python3
"""
üöÄ Model Trainer - –°–∏—Å—Ç–µ–º–∞ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º –æ—Ç–±–æ—Ä–æ–º –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏ —Å–ø—Ä–∞–≤–µ–¥–ª–∏–≤—É—é –≤–∞–ª–∏–¥–∞—Ü–∏—é —Å –æ–∫–Ω–∞–º–∏
–î–æ–±–∞–≤–ª–µ–Ω—ã: Optuna –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
–ù–û–í–´–ï –£–õ–£–ß–®–ï–ù–ò–Ø: Stacking, Voting, Deep Learning, ASHA/BOHB, TA-Lib, GPU –ø–æ–¥–¥–µ—Ä–∂–∫–∞
"""

import os
import psutil

import pandas as pd
import numpy as np
import warnings
warnings.filterwarnings('ignore')

# ML –±–∏–±–ª–∏–æ—Ç–µ–∫–∏
from sklearn.model_selection import TimeSeriesSplit
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
from sklearn.feature_selection import SelectKBest, f_classif, RFE, SelectFromModel
from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, VotingClassifier, StackingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neural_network import MLPClassifier

# –ì—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π –±—É—Å—Ç–∏–Ω–≥
import xgboost as xgb
import lightgbm as lgb
import catboost as cb

# –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏
from ngboost import NGBClassifier
from sklearn.ensemble import HistGradientBoostingClassifier

# SMOTE –¥–ª—è –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏
from imblearn.over_sampling import SMOTE, ADASYN, BorderlineSMOTE
from imblearn.under_sampling import RandomUnderSampler
from imblearn.combine import SMOTEENN, SMOTETomek

# Optuna –¥–ª—è –±–∞–π–µ—Å–æ–≤—Å–∫–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
import optuna
from optuna.samplers import TPESampler, CmaEsSampler
from optuna.pruners import HyperbandPruner, MedianPruner, SuccessiveHalvingPruner

# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import os
import json
import pickle
import joblib

# –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏
try:
    import talib
    TALIB_AVAILABLE = True
except ImportError:
    TALIB_AVAILABLE = False
    print("‚ö†Ô∏è TA-Lib –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install TA-Lib")

try:
    import torch
    import torch.nn as nn
    import torch.optim as optim
    from torch.utils.data import DataLoader, TensorDataset
    PYTORCH_AVAILABLE = True
except ImportError:
    PYTORCH_AVAILABLE = False
    print("‚ö†Ô∏è PyTorch –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install torch")

# GPU –ø–æ–¥–¥–µ—Ä–∂–∫–∞
try:
    import cupy as cp
    GPU_AVAILABLE = True
    print("üöÄ GPU –ø–æ–¥–¥–µ—Ä–∂–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–∞ (CuPy)")
except ImportError:
    GPU_AVAILABLE = False
    print("‚ö†Ô∏è GPU –ø–æ–¥–¥–µ—Ä–∂–∫–∞ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install cupy-cuda11x")

# –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ
from functools import lru_cache
import hashlib

class DeepNeuralNetwork(nn.Module):
    """
    –ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è –Ω–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å –¥–ª—è —Ç–æ—Ä–≥–æ–≤–ª–∏
    """
    def __init__(self, input_size, hidden_sizes=[256, 128, 64], dropout=0.3):
        super(DeepNeuralNetwork, self).__init__()
        layers = []
        prev_size = input_size
        
        for hidden_size in hidden_sizes:
            layers.extend([
                nn.Linear(prev_size, hidden_size),
                nn.BatchNorm1d(hidden_size),
                nn.ReLU(),
                nn.Dropout(dropout)
            ])
            prev_size = hidden_size
        
        layers.append(nn.Linear(prev_size, 3))  # 3 –∫–ª–∞—Å—Å–∞
        self.network = nn.Sequential(*layers)
    
    def forward(self, x):
        return self.network(x)

class ModelTrainer:
    """
    –°–∏—Å—Ç–µ–º–∞ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º –æ—Ç–±–æ—Ä–æ–º –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    """
    
    def __init__(self, timeframe='5m'):
        self.timeframe = timeframe
        self.models = {}
        self.feature_importance = {}
        self.results = {}
        self.scaler = RobustScaler()  # –ë–æ–ª–µ–µ —É—Å—Ç–æ–π—á–∏–≤—ã–π –∫ –≤—ã–±—Ä–æ—Å–∞–º
        self.best_params = {}
        
        # –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑ Optuna (–ù–ï –ò–ó–ú–ï–ù–Ø–ï–ú!)
        self.optimal_params = {
            '5m': {'percent': 0.75, 'horizon': 10},
            '15m': {'percent': 0.80, 'horizon': 7}
        }
        
        # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫–∏ –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        os.makedirs('model_results', exist_ok=True)
        os.makedirs('model_results/models', exist_ok=True)
        os.makedirs('model_results/features', exist_ok=True)
        os.makedirs('model_results/validation', exist_ok=True)
        os.makedirs('model_results/optuna', exist_ok=True)
        os.makedirs('model_results/ensemble', exist_ok=True)
        os.makedirs('model_results/deep_learning', exist_ok=True)
        
        # –ö—ç—à –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
        self._feature_cache = {}
        self._data_cache = {}
        
        print(f"üöÄ Model Trainer –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω –¥–ª—è {timeframe}")
        print(f"   –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: {self.optimal_params[timeframe]}")
        print(f"   TA-Lib: {'‚úÖ' if TALIB_AVAILABLE else '‚ùå'}")
        print(f"   PyTorch: {'‚úÖ' if PYTORCH_AVAILABLE else '‚ùå'}")
        print(f"   GPU: {'‚úÖ' if GPU_AVAILABLE else '‚ùå'}")
    
    @lru_cache(maxsize=128)
    def _get_cached_data(self, filename):
        """
        –ö—ç—à–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        """
        return pd.read_csv(filename)
    
    def load_data(self):
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è (2021-2024)
        """
        if self.timeframe == '5m':
            filename = '../data/historical/BTCUSDT_5m_5years_20210705_20250702.csv'
        else:
            filename = '../data/historical/BTCUSDT_15m_5years_20210705_20250702.csv'
        
        print(f"üìä –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö: {filename}")
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫—ç—à
        cache_key = hashlib.md5(filename.encode()).hexdigest()
        if cache_key in self._data_cache:
            df = self._data_cache[cache_key]
            print("‚úÖ –î–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏–∑ –∫—ç—à–∞")
        else:
            df = self._get_cached_data(filename)
            self._data_cache[cache_key] = df
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º timestamp
        if 'datetime' in df.columns:
            df['timestamp'] = pd.to_datetime(df['datetime'])
        else:
            df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')
        
        df = df.sort_values('timestamp').reset_index(drop=True)
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –¥–∞–Ω–Ω—ã–µ –¥–æ 2025 –≥–æ–¥–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        train_end_date = '2025-01-01'
        df_train = df[df['timestamp'] < train_end_date].copy()
        
        print(f"‚úÖ –î–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã: {len(df)} –∑–∞–ø–∏—Å–µ–π")
        print(f"   –ü–æ–ª–Ω—ã–π –ø–µ—Ä–∏–æ–¥: {df['timestamp'].min()} - {df['timestamp'].max()}")
        print(f"   –û–±—É—á–∞—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ: {len(df_train)} –∑–∞–ø–∏—Å–µ–π (–¥–æ {train_end_date})")
        print(f"   –ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è: –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è TimeSeriesSplit –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è")
        
        return df_train
    
    def create_features(self, df):
        """
        –°–æ–∑–¥–∞–µ—Ç –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –Ω–∞–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å TA-Lib –∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏
        """
        print("üîß –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")
        
        # –ë–∞–∑–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ —Ü–µ–Ω—ã
        df['returns'] = df['close'].pct_change()
        df['log_returns'] = np.log(df['close'] / df['close'].shift(1))
        df['high_low_ratio'] = df['high'] / df['low']
        df['close_open_ratio'] = df['close'] / df['open']
        
        # –í–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å
        df['volatility_5'] = df['returns'].rolling(5).std()
        df['volatility_10'] = df['returns'].rolling(10).std()
        df['volatility_20'] = df['returns'].rolling(20).std()
        
        # TA-Lib –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω—ã)
        if TALIB_AVAILABLE:
            print("   –î–æ–±–∞–≤–ª–µ–Ω–∏–µ TA-Lib –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤...")
            
            # RSI
            df['rsi_14'] = talib.RSI(df['close'].values, timeperiod=14)
            df['rsi_7'] = talib.RSI(df['close'].values, timeperiod=7)
            
            # MACD
            macd, macdsignal, macdhist = talib.MACD(df['close'].values)
            df['macd_line'] = macd
            df['macd_signal'] = macdsignal
            df['macd_histogram'] = macdhist
            
            # Moving Averages
            for period in [5, 10, 12, 20, 26, 50]:
                df[f'sma_{period}'] = df['close'].rolling(period).mean()
                df[f'ema_{period}'] = df['close'].ewm(span=period).mean()
                df[f'price_sma_{period}_ratio'] = df['close'] / df[f'sma_{period}']
            
            # Bollinger Bands
            bb_upper, bb_middle, bb_lower = talib.BBANDS(df['close'].values, timeperiod=20)
            df['bb_upper'] = bb_upper
            df['bb_middle'] = bb_middle
            df['bb_lower'] = bb_lower
            df['bb_position'] = (df['close'] - df['bb_lower']) / (df['bb_upper'] - df['bb_lower'])
            df['bb_width'] = (df['bb_upper'] - df['bb_lower']) / df['bb_middle']
            
            # Stochastic
            for period in [14, 7]:
                slowk, slowd = talib.STOCH(df['high'].values, df['low'].values, df['close'].values, 
                                         fastk_period=period, slowk_period=3, slowd_period=3)
                df[f'stoch_k_{period}'] = slowk
                df[f'stoch_d_{period}'] = slowd
            
            # Williams %R
            for period in [14, 7]:
                df[f'williams_r_{period}'] = talib.WILLR(df['high'].values, df['low'].values, df['close'].values, timeperiod=period)
            
            # CCI
            for period in [20, 10]:
                df[f'cci_{period}'] = talib.CCI(df['high'].values, df['low'].values, df['close'].values, timeperiod=period)
            
            # ATR
            for period in [14, 7]:
                df[f'atr_{period}'] = talib.ATR(df['high'].values, df['low'].values, df['close'].values, timeperiod=period)
                df[f'atr_ratio_{period}'] = df[f'atr_{period}'] / df['close']
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
            df['adx'] = talib.ADX(df['high'].values, df['low'].values, df['close'].values, timeperiod=14)
            df['obv'] = talib.OBV(df['close'].values, df['volume'].values)
            df['mfi'] = talib.MFI(df['high'].values, df['low'].values, df['close'].values, df['volume'].values, timeperiod=14)
            
            # Momentum
            for period in [5, 10, 20]:
                df[f'momentum_{period}'] = talib.MOM(df['close'].values, timeperiod=period)
                df[f'roc_{period}'] = talib.ROC(df['close'].values, timeperiod=period)
            
            # Volume indicators
            df['volume_sma'] = talib.SMA(df['volume'].values, timeperiod=20)
            df['volume_ratio'] = df['volume'] / df['volume_sma']
            df['ad'] = talib.AD(df['high'].values, df['low'].values, df['close'].values, df['volume'].values)
            
        else:
            # Fallback –±–µ–∑ TA-Lib
            print("   –ò—Å–ø–æ–ª—å–∑—É–µ–º –±–∞–∑–æ–≤—ã–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã (TA-Lib –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω)...")
            
            # RSI
            delta = df['close'].diff()
            gain = (delta.where(delta > 0, 0)).rolling(14).mean()
            loss = (-delta.where(delta < 0, 0)).rolling(14).mean()
            rs = gain / loss
            df['rsi_14'] = 100 - (100 / (1 + rs))
            df['rsi_7'] = 100 - (100 / (1 + (gain.rolling(7).mean() / loss.rolling(7).mean())))
            
            # MACD
            ema_12 = df['close'].ewm(span=12).mean()
            ema_26 = df['close'].ewm(span=26).mean()
            df['macd_line'] = ema_12 - ema_26
            df['macd_signal'] = df['macd_line'].ewm(span=9).mean()
            df['macd_histogram'] = df['macd_line'] - df['macd_signal']
            
            # Moving Averages
            for period in [5, 10, 12, 20, 26, 50]:
                df[f'sma_{period}'] = df['close'].rolling(period).mean()
                df[f'ema_{period}'] = df['close'].ewm(span=period).mean()
                df[f'price_sma_{period}_ratio'] = df['close'] / df[f'sma_{period}']
            
            # Bollinger Bands
            bb_period = 20
            bb_std = 2
            df['bb_middle'] = df['close'].rolling(bb_period).mean()
            bb_std_dev = df['close'].rolling(bb_period).std()
            df['bb_upper'] = df['bb_middle'] + (bb_std_dev * bb_std)
            df['bb_lower'] = df['bb_middle'] - (bb_std_dev * bb_std)
            df['bb_position'] = (df['close'] - df['bb_lower']) / (df['bb_upper'] - df['bb_lower'])
            df['bb_width'] = (df['bb_upper'] - df['bb_lower']) / df['bb_middle']
            
            # Volume indicators
            df['volume_sma'] = df['volume'].rolling(20).mean()
            df['volume_ratio'] = df['volume'] / df['volume_sma']
            df['volume_price_trend'] = (df['volume'] * df['returns']).rolling(10).sum()
            
            # Momentum indicators
            for period in [5, 10, 20]:
                df[f'momentum_{period}'] = df['close'].diff(period)
                df[f'roc_{period}'] = ((df['close'] - df['close'].shift(period)) / df['close'].shift(period) * 100)
            
            # Stochastic
            for period in [14, 7]:
                lowest_low = df['low'].rolling(period).min()
                highest_high = df['high'].rolling(period).max()
                df[f'stoch_k_{period}'] = 100 * ((df['close'] - lowest_low) / (highest_high - lowest_low))
                df[f'stoch_d_{period}'] = df[f'stoch_k_{period}'].rolling(3).mean()
            
            # Williams %R
            for period in [14, 7]:
                highest_high = df['high'].rolling(period).max()
                lowest_low = df['low'].rolling(period).min()
                df[f'williams_r_{period}'] = -100 * ((highest_high - df['close']) / (highest_high - lowest_low))
            
            # CCI
            for period in [20, 10]:
                typical_price = (df['high'] + df['low'] + df['close']) / 3
                sma_tp = typical_price.rolling(period).mean()
                mad = typical_price.rolling(period).apply(lambda x: np.mean(np.abs(x - x.mean())) if len(x) > 0 else 0)
                df[f'cci_{period}'] = (typical_price - sma_tp) / (0.015 * mad)
            
            # ATR
            for period in [14, 7]:
                high_low = df['high'] - df['low']
                high_close = np.abs(df['high'] - df['close'].shift())
                low_close = np.abs(df['low'] - df['close'].shift())
                true_range = np.maximum(high_low, np.maximum(high_close, low_close))
                df[f'atr_{period}'] = true_range.rolling(period).mean()
                df[f'atr_ratio_{period}'] = df[f'atr_{period}'] / df['close']
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        print("   –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")
        
        # Rolling statistics
        for period in [5, 10, 20]:
            df[f'returns_mean_{period}'] = df['returns'].rolling(period).mean()
            df[f'returns_std_{period}'] = df['returns'].rolling(period).std()
            df[f'returns_skew_{period}'] = df['returns'].rolling(period).skew()
            df[f'returns_kurt_{period}'] = df['returns'].rolling(period).kurt()
            df[f'returns_var_{period}'] = df['returns'].rolling(period).var()
            df[f'returns_median_{period}'] = df['returns'].rolling(period).median()
            
            # Volume statistics
            df[f'volume_mean_{period}'] = df['volume'].rolling(period).mean()
            df[f'volume_std_{period}'] = df['volume'].rolling(period).std()
            df[f'volume_skew_{period}'] = df['volume'].rolling(period).skew()
        
        # Lag features
        for lag in [1, 2, 3, 5, 10]:
            df[f'returns_lag_{lag}'] = df['returns'].shift(lag)
            df[f'volume_lag_{lag}'] = df['volume'].shift(lag)
            df[f'rsi_lag_{lag}'] = df['rsi_14'].shift(lag)
            df[f'close_lag_{lag}'] = df['close'].shift(lag)
        
        # –í—Ä–µ–º–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        df['hour'] = df['timestamp'].dt.hour
        df['day_of_week'] = df['timestamp'].dt.dayofweek
        df['month'] = df['timestamp'].dt.month
        df['day_of_month'] = df['timestamp'].dt.day
        
        # –¶–∏–∫–ª–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)
        df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)
        df['day_sin'] = np.sin(2 * np.pi * df['day_of_week'] / 7)
        df['day_cos'] = np.cos(2 * np.pi * df['day_of_week'] / 7)
        df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)
        df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)
        
        # –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        print("   –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")
        
        # Cross-over –ø—Ä–∏–∑–Ω–∞–∫–∏
        df['sma_cross_5_20'] = (df['sma_5'] > df['sma_20']).astype(int)
        df['ema_cross_12_26'] = (df['ema_12'] > df['ema_26']).astype(int)
        df['price_above_sma_20'] = (df['close'] > df['sma_20']).astype(int)
        
        # –í–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        df['volatility_ratio_5_20'] = df['volatility_5'] / df['volatility_20']
        df['volatility_change'] = df['volatility_10'].pct_change()
        
        # Momentum –ø—Ä–∏–∑–Ω–∞–∫–∏
        df['momentum_acceleration'] = df['momentum_5'].diff()
        df['rsi_momentum'] = df['rsi_14'].diff()
        df['macd_momentum'] = df['macd_line'].diff()
        
        # Volume-price –ø—Ä–∏–∑–Ω–∞–∫–∏
        df['volume_price_correlation'] = df['volume'].rolling(10).corr(df['close'])
        df['volume_returns_correlation'] = df['volume'].rolling(10).corr(df['returns'])
        
        # Support/Resistance –ø—Ä–∏–∑–Ω–∞–∫–∏
        df['support_level'] = df['low'].rolling(20).min()
        df['resistance_level'] = df['high'].rolling(20).max()
        df['price_to_support'] = (df['close'] - df['support_level']) / df['close']
        df['price_to_resistance'] = (df['resistance_level'] - df['close']) / df['close']
        
        print(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ {len(df.columns)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
        return df
    
    def create_target(self, df):
        """
        –°–æ–∑–¥–∞–µ—Ç —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é —Å 3 –∫–ª–∞—Å—Å–∞–º–∏ - –ü–†–Ø–ú–ê–Ø –°–í–Ø–ó–¨ —Å —Ç–æ—Ä–≥–æ–≤—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
        –ú–æ–¥–µ–ª—å —É—á–∏—Ç—Å—è –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—Ç—å –∏–º–µ–Ω–Ω–æ —Ç–µ –¥–≤–∏–∂–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–µ –Ω—É–∂–Ω—ã –¥–ª—è —Ç–æ—Ä–≥–æ–≤–ª–∏
        """
        print("üéØ –°–æ–∑–¥–∞–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π —Å –ø—Ä—è–º–æ–π —Å–≤—è–∑—å—é —Å —Ç–æ—Ä–≥–æ–≤–ª–µ–π...")
        
        # –ü–†–Ø–ú–ê–Ø –°–í–Ø–ó–¨: –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ –∂–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã —á—Ç–æ –∏ –¥–ª—è —Ç–æ—Ä–≥–æ–≤–ª–∏
        if self.timeframe == '5m':
            horizon = 10
            tp_percent = 0.75  # Take Profit = —Ü–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è
        else:
            horizon = 7
            tp_percent = 0.80  # Take Profit = —Ü–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è
        
        print(f"‚úÖ –ü–†–Ø–ú–ê–Ø –°–í–Ø–ó–¨ —Å —Ç–æ—Ä–≥–æ–≤—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏:")
        print(f"   –¢–∞–π–º—Ñ—Ä–µ–π–º: {self.timeframe}")
        print(f"   –ì–æ—Ä–∏–∑–æ–Ω—Ç: {horizon} —Å–≤–µ—á–µ–π")
        print(f"   Take Profit = –¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è: {tp_percent}%")
        print(f"   –ú–æ–¥–µ–ª—å —É—á–∏—Ç—Å—è –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—Ç—å –∏–º–µ–Ω–Ω–æ {tp_percent}% –¥–≤–∏–∂–µ–Ω–∏—è!")
        
        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –¥–≤–∏–∂–µ–Ω–∏–µ —Ü–µ–Ω—ã –≤ –≥–æ—Ä–∏–∑–æ–Ω—Ç–µ
        future_high = df['high'].rolling(horizon, min_periods=1).max().shift(-horizon)
        future_low = df['low'].rolling(horizon, min_periods=1).min().shift(-horizon)
        
        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—É—é –ø—Ä–∏–±—ã–ª—å –∏ —É–±—ã—Ç–æ–∫
        potential_profit_long = (future_high - df['close']) / df['close'] * 100
        potential_profit_short = (df['close'] - future_low) / df['close'] * 100
        
        # –ü–†–Ø–ú–ê–Ø –°–í–Ø–ó–¨: –∏—Å–ø–æ–ª—å–∑—É–µ–º tp_percent –∫–∞–∫ –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω—ã–π –ø–æ—Ä–æ–≥
        tp_threshold = tp_percent  # Take Profit = —Ü–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è
        sl_threshold = tp_percent * 0.67  # Stop Loss (1/1.5 –æ—Ç TP)
        
        df['target'] = 0  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ
        
        # –°–∏–≥–Ω–∞–ª –Ω–∞ –ª–æ–Ω–≥: –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–∞—è –ø—Ä–∏–±—ã–ª—å >= TP –ò –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–π —É–±—ã—Ç–æ–∫ <= SL
        long_condition = (
            (potential_profit_long >= tp_threshold) &
            (potential_profit_short <= sl_threshold)
        )
        df.loc[long_condition, 'target'] = 1
        
        # –°–∏–≥–Ω–∞–ª –Ω–∞ —à–æ—Ä—Ç: –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–∞—è –ø—Ä–∏–±—ã–ª—å –ø–æ —à–æ—Ä—Ç—É >= TP –ò –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–π —É–±—ã—Ç–æ–∫ <= SL
        short_condition = (
            (potential_profit_short >= tp_threshold) &
            (potential_profit_long <= sl_threshold)
        )
        df.loc[short_condition, 'target'] = -1
        
        # –£–±–∏—Ä–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ –∑–∞–ø–∏—Å–∏ –≥–¥–µ –Ω–µ—Ç –±—É–¥—É—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö
        df = df.iloc[:-horizon].copy()
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –º–µ—Ç–∫–∏ –∫–ª–∞—Å—Å–æ–≤ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å –º–æ–¥–µ–ª—è–º–∏: -1->0, 0->1, 1->2
        df['target_original'] = df['target'].copy()  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –º–µ—Ç–∫–∏
        df['target'] = df['target'].map({-1: 0, 0: 1, 1: 2})  # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫–ª–∞—Å—Å–æ–≤
        long_samples = (df['target_original'] == 1).sum()
        short_samples = (df['target_original'] == -1).sum()
        neutral_samples = (df['target_original'] == 0).sum()
        total_samples = len(df)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –≤ –∞—Ç—Ä–∏–±—É—Ç—ã –∫–ª–∞—Å—Å–∞
        self.long_samples = long_samples
        self.short_samples = short_samples
        self.neutral_samples = neutral_samples
        self.total_samples = total_samples
        
        print(f"‚úÖ –¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è —Å–æ–∑–¥–∞–Ω–∞ (–ü–†–Ø–ú–ê–Ø –°–í–Ø–ó–¨):")
        print(f"   Take Profit = –¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è: {tp_threshold:.2f}% –∑–∞ {horizon} —Å–≤–µ—á–µ–π")
        print(f"   Stop Loss: {sl_threshold:.2f}%")
        print(f"   –°–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ TP/SL: 1:1.5")
        print(f"")
        print(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–∏–≥–Ω–∞–ª–æ–≤:")
        print(f"   –õ–æ–Ω–≥ —Å–∏–≥–Ω–∞–ª—ã: {long_samples} ({long_samples/total_samples*100:.1f}%)")
        print(f"   –®–æ—Ä—Ç —Å–∏–≥–Ω–∞–ª—ã: {short_samples} ({short_samples/total_samples*100:.1f}%)")
        print(f"   –ù–µ–π—Ç—Ä–∞–ª—å–Ω—ã–µ: {neutral_samples} ({neutral_samples/total_samples*100:.1f}%)")
        print(f"   –í—Å–µ–≥–æ –ø—Ä–∏–º–µ—Ä–æ–≤: {total_samples}")
        print(f"")
        print(f"üéØ –ú–æ–¥–µ–ª—å —É—á–∏—Ç—Å—è –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—Ç—å –∏–º–µ–Ω–Ω–æ {tp_threshold}% –¥–≤–∏–∂–µ–Ω–∏—è!")
        print(f"üìà –ú–µ—Ç–∫–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è: 0 (—à–æ—Ä—Ç), 1 (–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ), 2 (–ª–æ–Ω–≥)")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        self.target_params = {
            'horizon': horizon,
            'tp_threshold': tp_threshold,
            'sl_threshold': sl_threshold,
            'tp_sl_ratio': 1.5
        }
        
        return df
    
    def select_features(self, X, y, method='kbest', n_features=50):
        """
        –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –æ—Ç–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        """
        print(f"üîç –û—Ç–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –º–µ—Ç–æ–¥–æ–º {method}...")
        
        if method == 'kbest':
            selector = SelectKBest(score_func=f_classif, k=n_features)
        elif method == 'rfe':
            estimator = RandomForestClassifier(n_estimators=100, random_state=42)
            selector = RFE(estimator=estimator, n_features_to_select=n_features)
        else:
            return X, list(X.columns)
        
        X_selected = selector.fit_transform(X, y)
        selected_features = X.columns[selector.get_support()].tolist()
        
        print(f"‚úÖ –û—Ç–æ–±—Ä–∞–Ω–æ {len(selected_features)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ {len(X.columns)}")
        
        return X_selected, selected_features
    
    def get_models_for_optuna(self):
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –º–æ–¥–µ–ª–∏ —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –¥–ª—è Optuna –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ (–ø–æ–¥–¥–µ—Ä–∂–∫–∞ 3 –∫–ª–∞—Å—Å–æ–≤)
        –í–∫–ª—é—á–∞–µ—Ç: –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –º–æ–¥–µ–ª–∏, –∞–Ω—Å–∞–º–±–ª–∏, –≥–ª—É–±–æ–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ
        """
        models = {
            # –ì—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π –±—É—Å—Ç–∏–Ω–≥ —Å –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
            'XGBoost': {
                'model': xgb.XGBClassifier(random_state=42, eval_metric='mlogloss', use_label_encoder=False, objective='multi:softprob', num_class=3, tree_method='gpu_hist' if GPU_AVAILABLE else 'hist'),
                'params': {
                    'n_estimators': [100, 200, 300],
                    'max_depth': [4, 6, 8],
                    'learning_rate': [0.05, 0.1, 0.2],
                    'subsample': [0.8, 0.9, 1.0],
                    'colsample_bytree': [0.8, 0.9, 1.0],
                    'reg_alpha': [0.001, 0.01, 0.1, 1.0],
                    'reg_lambda': [0.001, 0.01, 0.1, 1.0],
                    'min_child_weight': [1, 3, 5, 7]
                }
            },
            'LightGBM': {
                'model': lgb.LGBMClassifier(random_state=42, verbose=-1, objective='multiclass', device='gpu' if GPU_AVAILABLE else 'cpu'),
                'params': {
                    'n_estimators': [100, 200, 300],
                    'max_depth': [4, 6, 8],
                    'learning_rate': [0.05, 0.1, 0.2],
                    'subsample': [0.8, 0.9, 1.0],
                    'colsample_bytree': [0.8, 0.9, 1.0],
                    'reg_alpha': [0.001, 0.01, 0.1, 1.0],
                    'reg_lambda': [0.001, 0.01, 0.1, 1.0],
                    'min_child_samples': [10, 20, 50, 100]
                }
            },
            'CatBoost': {
                'model': cb.CatBoostClassifier(random_state=42, verbose=False, loss_function='MultiClass', task_type='GPU' if GPU_AVAILABLE else 'CPU'),
                'params': {
                    'iterations': [100, 200, 300],
                    'depth': [4, 6, 8],
                    'learning_rate': [0.05, 0.1, 0.2],
                    'l2_leaf_reg': [1, 3, 5, 10],
                    'border_count': [32, 64, 128]
                }
            },
            
            # –ê–Ω—Å–∞–º–±–ª–µ–≤—ã–µ –º–µ—Ç–æ–¥—ã —Å –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
            'RandomForest': {
                'model': RandomForestClassifier(random_state=42),
                'params': {
                    'n_estimators': [100, 200, 300],
                    'max_depth': [8, 10, 12, 15],
                    'min_samples_split': [2, 5, 10],
                    'min_samples_leaf': [1, 2, 4],
                    'max_features': ['sqrt', 'log2', None]
                }
            },
            'ExtraTrees': {
                'model': ExtraTreesClassifier(random_state=42),
                'params': {
                    'n_estimators': [100, 200, 300],
                    'max_depth': [8, 10, 12, 15],
                    'min_samples_split': [2, 5, 10],
                    'min_samples_leaf': [1, 2, 4],
                    'max_features': ['sqrt', 'log2', None]
                }
            },
            'HistGradientBoosting': {
                'model': HistGradientBoostingClassifier(random_state=42),
                'params': {
                    'max_iter': [100, 200, 300],
                    'max_depth': [4, 6, 8],
                    'learning_rate': [0.05, 0.1, 0.2],
                    'l2_regularization': [0.1, 0.5, 1.0],
                    'min_samples_leaf': [10, 20, 50]
                }
            },
            
            # –ù–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å —Å –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
            'MLP': {
                'model': MLPClassifier(max_iter=1000, random_state=42),
                'params': {
                    'hidden_layer_sizes': [(50,), (100,), (100, 50), (200, 100), (200, 100, 50)],
                    'alpha': [0.001, 0.01, 0.1],
                    'learning_rate_init': [0.001, 0.01, 0.1]
                }
            }
        }
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –º–æ–¥–µ–ª–∏ –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω—ã
        if PYTORCH_AVAILABLE:
            models['DeepNeuralNetwork'] = {
                'model': 'custom',  # –°–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞
                'params': {
                    'hidden_sizes': [[128, 64], [256, 128, 64], [512, 256, 128, 64]],
                    'dropout': [0.2, 0.3, 0.4],
                    'learning_rate': [0.001, 0.01],
                    'batch_size': [32, 64, 128]
                }
            }
        
        return models
    
    def create_ensemble_models(self, base_models):
        """
        –°–æ–∑–¥–∞–µ—Ç –∞–Ω—Å–∞–º–±–ª–µ–≤—ã–µ –º–æ–¥–µ–ª–∏ (Stacking, Voting)
        """
        print("üéØ –°–æ–∑–¥–∞–Ω–∏–µ –∞–Ω—Å–∞–º–±–ª–µ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π...")
        
        ensemble_models = {}
        
        # Voting Classifier (Hard Voting)
        voting_hard = VotingClassifier(
            estimators=[(name, model) for name, model in base_models.items()],
            voting='hard'
        )
        ensemble_models['VotingHard'] = {
            'model': voting_hard,
            'params': {}
        }
        
        # Voting Classifier (Soft Voting)
        voting_soft = VotingClassifier(
            estimators=[(name, model) for name, model in base_models.items()],
            voting='soft'
        )
        ensemble_models['VotingSoft'] = {
            'model': voting_soft,
            'params': {}
        }
        
        # Stacking Classifier
        estimators = [(name, model) for name, model in base_models.items()]
        stacking = StackingClassifier(
            estimators=estimators,
            final_estimator=LogisticRegression(random_state=42),
            cv=3
        )
        ensemble_models['Stacking'] = {
            'model': stacking,
            'params': {}
        }
        
        print(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ {len(ensemble_models)} –∞–Ω—Å–∞–º–±–ª–µ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π")
        return ensemble_models
    
    def create_model_with_params(self, model_name, params):
        """
        –°–æ–∑–¥–∞–µ—Ç –º–æ–¥–µ–ª—å —Å –∑–∞–¥–∞–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
        """
        try:
            if model_name == 'XGBoost':
                return xgb.XGBClassifier(**params, random_state=42, eval_metric='mlogloss', use_label_encoder=False, objective='multi:softprob', num_class=3, tree_method='gpu_hist' if GPU_AVAILABLE else 'hist')
            elif model_name == 'LightGBM':
                return lgb.LGBMClassifier(**params, random_state=42, verbose=-1, objective='multiclass', device='gpu' if GPU_AVAILABLE else 'cpu')
            elif model_name == 'CatBoost':
                return cb.CatBoostClassifier(**params, random_state=42, verbose=False, loss_function='MultiClass', task_type='GPU' if GPU_AVAILABLE else 'CPU')
            elif model_name == 'RandomForest':
                # –ï—Å–ª–∏ bootstrap=False, —É–¥–∞–ª—è–µ–º max_samples –∏–∑ params
                if 'bootstrap' in params and not params['bootstrap']:
                    params.pop('max_samples', None)
                return RandomForestClassifier(**params, random_state=42)
            elif model_name == 'ExtraTrees':
                # –ï—Å–ª–∏ bootstrap=False, —É–¥–∞–ª—è–µ–º max_samples –∏–∑ params
                if 'bootstrap' in params and not params['bootstrap']:
                    params.pop('max_samples', None)
                return ExtraTreesClassifier(**params, random_state=42)
            elif model_name == 'HistGradientBoosting':
                return HistGradientBoostingClassifier(**params, random_state=42)
            elif model_name == 'MLP':
                # –£–±–∏—Ä–∞–µ–º max_iter –∏–∑ params –µ—Å–ª–∏ –æ–Ω —Ç–∞–º –µ—Å—Ç—å, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è
                if 'max_iter' in params:
                    del params['max_iter']
                return MLPClassifier(**params, max_iter=500, random_state=42)
            else:
                raise ValueError(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –º–æ–¥–µ–ª—å: {model_name}")
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –º–æ–¥–µ–ª–∏ {model_name}: {e}")
            print(f"   –ü–∞—Ä–∞–º–µ—Ç—Ä—ã: {params}")
            import traceback
            traceback.print_exc()
            return None
    
    def optuna_objective(self, trial, X_train, X_val, y_train, y_val, model_name):
        """
        –¶–µ–ª–µ–≤–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è Optuna —Å –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–º–∏ –∞–ª–≥–æ—Ä–∏—Ç–º–∞–º–∏ (–ø–æ–¥–¥–µ—Ä–∂–∫–∞ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π)
        –£–ª—É—á—à–µ–Ω–æ –¥–ª—è –±–æ–ª–µ–µ —Ç—â–∞—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Å —É–≤–µ–ª–∏—á–µ–Ω–Ω–æ–π —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π
        """
        try:
            if model_name == 'XGBoost':
                params = {
                    'n_estimators': trial.suggest_int('n_estimators', 200, 1000),  # –£–≤–µ–ª–∏—á–µ–Ω–æ
                    'max_depth': trial.suggest_int('max_depth', 4, 12),  # –†–∞—Å—à–∏—Ä–µ–Ω –¥–∏–∞–ø–∞–∑–æ–Ω
                    'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.3, log=True),  # –ë–æ–ª–µ–µ –º–µ–ª–∫–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è
                    'subsample': trial.suggest_float('subsample', 0.6, 1.0),
                    'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),
                    'reg_alpha': trial.suggest_float('reg_alpha', 0.001, 20, log=True),  # –†–∞—Å—à–∏—Ä–µ–Ω –¥–∏–∞–ø–∞–∑–æ–Ω
                    'reg_lambda': trial.suggest_float('reg_lambda', 0.001, 20, log=True),  # –†–∞—Å—à–∏—Ä–µ–Ω –¥–∏–∞–ø–∞–∑–æ–Ω
                    'min_child_weight': trial.suggest_int('min_child_weight', 1, 15),  # –†–∞—Å—à–∏—Ä–µ–Ω –¥–∏–∞–ø–∞–∑–æ–Ω
                    'gamma': trial.suggest_float('gamma', 0, 5),  # –î–æ–±–∞–≤–ª–µ–Ω gamma
                    'scale_pos_weight': trial.suggest_float('scale_pos_weight', 0.5, 3.0)  # –î–æ–±–∞–≤–ª–µ–Ω –¥–ª—è –Ω–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∫–ª–∞—Å—Å–æ–≤
                }
            elif model_name == 'LightGBM':
                params = {
                    'n_estimators': trial.suggest_int('n_estimators', 200, 1000),  # –£–≤–µ–ª–∏—á–µ–Ω–æ
                    'max_depth': trial.suggest_int('max_depth', 4, 12),  # –†–∞—Å—à–∏—Ä–µ–Ω –¥–∏–∞–ø–∞–∑–æ–Ω
                    'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.3, log=True),  # –ë–æ–ª–µ–µ –º–µ–ª–∫–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è
                    'subsample': trial.suggest_float('subsample', 0.6, 1.0),
                    'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),
                    'reg_alpha': trial.suggest_float('reg_alpha', 0.001, 20, log=True),  # –†–∞—Å—à–∏—Ä–µ–Ω –¥–∏–∞–ø–∞–∑–æ–Ω
                    'reg_lambda': trial.suggest_float('reg_lambda', 0.001, 20, log=True),  # –†–∞—Å—à–∏—Ä–µ–Ω –¥–∏–∞–ø–∞–∑–æ–Ω
                    'min_child_samples': trial.suggest_int('min_child_samples', 10, 200),  # –†–∞—Å—à–∏—Ä–µ–Ω –¥–∏–∞–ø–∞–∑–æ–Ω
                    'min_split_gain': trial.suggest_float('min_split_gain', 0, 1),  # –î–æ–±–∞–≤–ª–µ–Ω
                    'class_weight': trial.suggest_categorical('class_weight', ['balanced', None])  # –î–æ–±–∞–≤–ª–µ–Ω
                }
            elif model_name == 'CatBoost':
                params = {
                    'iterations': trial.suggest_int('iterations', 200, 1000),  # –£–≤–µ–ª–∏—á–µ–Ω–æ
                    'depth': trial.suggest_int('depth', 4, 12),  # –†–∞—Å—à–∏—Ä–µ–Ω –¥–∏–∞–ø–∞–∑–æ–Ω
                    'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.3, log=True),  # –ë–æ–ª–µ–µ –º–µ–ª–∫–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è
                    'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1, 30, log=True),  # –†–∞—Å—à–∏—Ä–µ–Ω –¥–∏–∞–ø–∞–∑–æ–Ω
                    'border_count': trial.suggest_categorical('border_count', [32, 64, 128, 254]),
                    'bagging_temperature': trial.suggest_float('bagging_temperature', 0, 2),  # –†–∞—Å—à–∏—Ä–µ–Ω –¥–∏–∞–ø–∞–∑–æ–Ω
                    'random_strength': trial.suggest_float('random_strength', 0, 10),  # –î–æ–±–∞–≤–ª–µ–Ω
                    'class_weights': trial.suggest_categorical('class_weights', [None, 'balanced'])  # –î–æ–±–∞–≤–ª–µ–Ω
                }
            elif model_name == 'RandomForest':
                params = {
                    'n_estimators': trial.suggest_int('n_estimators', 200, 1000),  # –£–≤–µ–ª–∏—á–µ–Ω–æ
                    'max_depth': trial.suggest_int('max_depth', 10, 25),  # –†–∞—Å—à–∏—Ä–µ–Ω –¥–∏–∞–ø–∞–∑–æ–Ω
                    'min_samples_split': trial.suggest_int('min_samples_split', 2, 30),  # –†–∞—Å—à–∏—Ä–µ–Ω –¥–∏–∞–ø–∞–∑–æ–Ω
                    'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 15),  # –†–∞—Å—à–∏—Ä–µ–Ω –¥–∏–∞–ø–∞–∑–æ–Ω
                    'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2', None]),
                    'class_weight': trial.suggest_categorical('class_weight', ['balanced', 'balanced_subsample', None]),  # –î–æ–±–∞–≤–ª–µ–Ω
                    'bootstrap': trial.suggest_categorical('bootstrap', [True, False]),  # –î–æ–±–∞–≤–ª–µ–Ω
                    'max_samples': trial.suggest_float('max_samples', 0.5, 1.0)  # –î–æ–±–∞–≤–ª–µ–Ω
                }
            elif model_name == 'ExtraTrees':
                params = {
                    'n_estimators': trial.suggest_int('n_estimators', 200, 1000),  # –£–≤–µ–ª–∏—á–µ–Ω–æ
                    'max_depth': trial.suggest_int('max_depth', 10, 25),  # –†–∞—Å—à–∏—Ä–µ–Ω –¥–∏–∞–ø–∞–∑–æ–Ω
                    'min_samples_split': trial.suggest_int('min_samples_split', 2, 30),  # –†–∞—Å—à–∏—Ä–µ–Ω –¥–∏–∞–ø–∞–∑–æ–Ω
                    'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 15),  # –†–∞—Å—à–∏—Ä–µ–Ω –¥–∏–∞–ø–∞–∑–æ–Ω
                    'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2', None]),
                    'class_weight': trial.suggest_categorical('class_weight', ['balanced', 'balanced_subsample', None]),  # –î–æ–±–∞–≤–ª–µ–Ω
                    'bootstrap': trial.suggest_categorical('bootstrap', [True, False]),  # –î–æ–±–∞–≤–ª–µ–Ω
                    'max_samples': trial.suggest_float('max_samples', 0.5, 1.0)  # –î–æ–±–∞–≤–ª–µ–Ω
                }
            elif model_name == 'HistGradientBoosting':
                params = {
                    'max_iter': trial.suggest_int('max_iter', 200, 1000),  # –£–≤–µ–ª–∏—á–µ–Ω–æ
                    'max_depth': trial.suggest_int('max_depth', 4, 12),  # –†–∞—Å—à–∏—Ä–µ–Ω –¥–∏–∞–ø–∞–∑–æ–Ω
                    'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.3, log=True),  # –ë–æ–ª–µ–µ –º–µ–ª–∫–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è
                    'l2_regularization': trial.suggest_float('l2_regularization', 0.1, 5.0, log=True),  # –†–∞—Å—à–∏—Ä–µ–Ω –¥–∏–∞–ø–∞–∑–æ–Ω
                    'min_samples_leaf': trial.suggest_int('min_samples_leaf', 10, 200),  # –†–∞—Å—à–∏—Ä–µ–Ω –¥–∏–∞–ø–∞–∑–æ–Ω
                    'class_weight': trial.suggest_categorical('class_weight', ['balanced', None])  # –î–æ–±–∞–≤–ª–µ–Ω
                }
            elif model_name == 'MLP':
                params = {
                    'hidden_layer_sizes': trial.suggest_categorical('hidden_layer_sizes', 
                        [(100,), (200,), (100, 50), (200, 100), (200, 100, 50), (300, 150, 75), (500, 250, 125)]),  # –î–æ–±–∞–≤–ª–µ–Ω—ã –±–æ–ª—å—à–∏–µ —Å–µ—Ç–∏
                    'alpha': trial.suggest_float('alpha', 0.0001, 0.1, log=True),  # –†–∞—Å—à–∏—Ä–µ–Ω –¥–∏–∞–ø–∞–∑–æ–Ω
                    'learning_rate_init': trial.suggest_float('learning_rate_init', 0.0001, 0.1, log=True),  # –†–∞—Å—à–∏—Ä–µ–Ω –¥–∏–∞–ø–∞–∑–æ–Ω
                    'max_iter': trial.suggest_int('max_iter', 500, 2000),  # –î–æ–±–∞–≤–ª–µ–Ω
                    'early_stopping': trial.suggest_categorical('early_stopping', [True, False]),  # –î–æ–±–∞–≤–ª–µ–Ω
                    'validation_fraction': trial.suggest_float('validation_fraction', 0.1, 0.2)  # –î–æ–±–∞–≤–ª–µ–Ω
                }
            elif model_name == 'DeepNeuralNetwork':
                params = {
                    'hidden_sizes': trial.suggest_categorical('hidden_sizes', 
                        [[128, 64], [256, 128, 64], [512, 256, 128, 64], [1024, 512, 256, 128], [2048, 1024, 512, 256]]),  # –î–æ–±–∞–≤–ª–µ–Ω—ã –±–æ–ª—å—à–∏–µ —Å–µ—Ç–∏
                    'dropout': trial.suggest_float('dropout', 0.1, 0.6),  # –†–∞—Å—à–∏—Ä–µ–Ω –¥–∏–∞–ø–∞–∑–æ–Ω
                    'learning_rate': trial.suggest_float('learning_rate', 0.0001, 0.01, log=True),  # –†–∞—Å—à–∏—Ä–µ–Ω –¥–∏–∞–ø–∞–∑–æ–Ω
                    'batch_size': trial.suggest_categorical('batch_size', [16, 32, 64, 128, 256]),  # –î–æ–±–∞–≤–ª–µ–Ω—ã –≤–∞—Ä–∏–∞–Ω—Ç—ã
                    'epochs': trial.suggest_int('epochs', 50, 200)  # –î–æ–±–∞–≤–ª–µ–Ω –ø–∞—Ä–∞–º–µ—Ç—Ä epochs
                }
            else:
                raise ValueError(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –º–æ–¥–µ–ª—å: {model_name}")
            
            # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å
            if model_name == 'DeepNeuralNetwork':
                model = self.create_deep_learning_model(X_train.shape[1], params)
            else:
                model = self.create_model_with_params(model_name, params)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –º–æ–¥–µ–ª—å —Å–æ–∑–¥–∞–Ω–∞
            if model is None:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞: –º–æ–¥–µ–ª—å {model_name} –Ω–µ —Å–æ–∑–¥–∞–Ω–∞")
                return 0.0
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—É—é –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫—É (–±–µ–∑ SMOTE)
            X_train_balanced, y_train_balanced = X_train, y_train
            
            # –û–±—É—á–µ–Ω–∏–µ —Å early stopping –¥–ª—è –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö –º–æ–¥–µ–ª–µ–π
            if model_name in ['XGBoost', 'LightGBM', 'CatBoost']:
                # Early stopping –¥–ª—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≥–æ –±—É—Å—Ç–∏–Ω–≥–∞
                eval_set = [(X_val, y_val)]
                if model_name == 'XGBoost':
                    model.fit(X_train_balanced, y_train_balanced, 
                             eval_set=eval_set, early_stopping_rounds=50, verbose=False)  # –£–≤–µ–ª–∏—á–µ–Ω–æ —Å 20 –¥–æ 50
                elif model_name == 'LightGBM':
                    model.fit(X_train_balanced, y_train_balanced,
                             eval_set=eval_set, callbacks=[lgb.early_stopping(50), lgb.log_evaluation(0)])  # –£–≤–µ–ª–∏—á–µ–Ω–æ —Å 20 –¥–æ 50
                elif model_name == 'CatBoost':
                    model.fit(X_train_balanced, y_train_balanced,
                             eval_set=(X_val, y_val), early_stopping_rounds=50, verbose=False)  # –£–≤–µ–ª–∏—á–µ–Ω–æ —Å 20 –¥–æ 50
            elif model_name == 'DeepNeuralNetwork':
                # –°–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è PyTorch
                score = self.train_deep_learning_model(model, X_train_balanced, X_val, y_train_balanced, y_val, params)
                return score
            else:
                # –û–±—ã—á–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ
                model.fit(X_train_balanced, y_train_balanced)
            
            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
            if model_name == 'DeepNeuralNetwork':
                y_pred = self.predict_deep_learning(model, X_val)
            else:
                y_pred = model.predict(X_val)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–µ –ø—É—Å—Ç—ã–µ
            if len(y_pred) == 0:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞: –ø—É—Å—Ç—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–ª—è {model_name}")
                return 0.0
            
            # –û—Ü–µ–Ω–∫–∞ —Å F1-score (–ª—É—á—à–µ –¥–ª—è –Ω–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∫–ª–∞—Å—Å–æ–≤)
            score = f1_score(y_val, y_pred, average='weighted')
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ score –Ω–µ nan
            if np.isnan(score):
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞: nan score –¥–ª—è {model_name}")
                return 0.0
            
            return score
            
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –≤ optuna_objective –¥–ª—è {model_name}: {e}")
            import traceback
            traceback.print_exc()
            return 0.0
    
    def create_deep_learning_model(self, input_size, params):
        """
        –°–æ–∑–¥–∞–µ—Ç –º–æ–¥–µ–ª—å –≥–ª—É–±–æ–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
        """
        if not PYTORCH_AVAILABLE:
            raise ValueError("PyTorch –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
        
        model = DeepNeuralNetwork(
            input_size=input_size,
            hidden_sizes=params['hidden_sizes'],
            dropout=params['dropout']
        )
        
        # GPU –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω
        if torch.cuda.is_available():
            model = model.cuda()
        
        return model
    
    def train_deep_learning_model(self, model, X_train, X_val, y_train, y_val, params):
        """
        –û–±—É—á–∞–µ—Ç –º–æ–¥–µ–ª—å –≥–ª—É–±–æ–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
        """
        if not PYTORCH_AVAILABLE:
            return 0.0
        
        try:
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –¥–∞–Ω–Ω—ã–µ –≤ numpy –º–∞—Å—Å–∏–≤—ã
            if hasattr(X_train, 'values'):
                X_train = X_train.values
            elif hasattr(X_train, 'numpy'):
                X_train = X_train.numpy()
            else:
                X_train = np.array(X_train)
                
            if hasattr(X_val, 'values'):
                X_val = X_val.values
            elif hasattr(X_val, 'numpy'):
                X_val = X_val.numpy()
            else:
                X_val = np.array(X_val)
                
            if hasattr(y_train, 'values'):
                y_train = y_train.values
            elif hasattr(y_train, 'numpy'):
                y_train = y_train.numpy()
            else:
                y_train = np.array(y_train)
                
            if hasattr(y_val, 'values'):
                y_val = y_val.values
            elif hasattr(y_val, 'numpy'):
                y_val = y_val.numpy()
            else:
                y_val = np.array(y_val)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –¥–∞–Ω–Ω—ã–µ –Ω–µ –ø—É—Å—Ç—ã–µ
            if len(X_train) == 0 or len(X_val) == 0:
                print("‚ö†Ô∏è –û—à–∏–±–∫–∞: –ø—É—Å—Ç—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è DeepNeuralNetwork")
                return 0.0
            
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –¥–∞–Ω–Ω—ã–µ –≤ —Ç–µ–Ω–∑–æ—Ä—ã
            X_train_tensor = torch.FloatTensor(X_train)
            y_train_tensor = torch.LongTensor(y_train)
            X_val_tensor = torch.FloatTensor(X_val)
            y_val_tensor = torch.LongTensor(y_val)
            
            # GPU –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω
            if torch.cuda.is_available():
                X_train_tensor = X_train_tensor.cuda()
                y_train_tensor = y_train_tensor.cuda()
                X_val_tensor = X_val_tensor.cuda()
                y_val_tensor = y_val_tensor.cuda()
            
            # DataLoader
            train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
            train_loader = DataLoader(train_dataset, batch_size=params['batch_size'], shuffle=True)
            
            # –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –∏ —Ñ—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å
            optimizer = optim.Adam(model.parameters(), lr=params['learning_rate'])
            criterion = nn.CrossEntropyLoss()
            
            # –û–±—É—á–µ–Ω–∏–µ
            model.train()
            best_score = 0.0
            patience = 10
            no_improve = 0
            
            for epoch in range(min(50, params.get('epochs', 50))):  # –ú–∞–∫—Å–∏–º—É–º 50 —ç–ø–æ—Ö –∏–ª–∏ –∏–∑ params
                epoch_loss = 0
                for batch_X, batch_y in train_loader:
                    optimizer.zero_grad()
                    outputs = model(batch_X)
                    loss = criterion(outputs, batch_y)
                    loss.backward()
                    optimizer.step()
                    epoch_loss += loss.item()
                
                # Early stopping –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
                if epoch % 5 == 0:
                    model.eval()
                    with torch.no_grad():
                        val_outputs = model(X_val_tensor)
                        val_pred = torch.argmax(val_outputs, dim=1)
                        score = f1_score(y_val_tensor.cpu().numpy(), val_pred.cpu().numpy(), average='weighted', zero_division=0)
                        
                        if score > best_score:
                            best_score = score
                            no_improve = 0
                        else:
                            no_improve += 1
                            
                        if no_improve >= patience:
                            break
                    model.train()
            
            return best_score
            
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –≤ train_deep_learning_model: {e}")
            import traceback
            traceback.print_exc()
            return 0.0
    
    def predict_deep_learning(self, model, X):
        """
        –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–ª—è –º–æ–¥–µ–ª–∏ –≥–ª—É–±–æ–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
        """
        if not PYTORCH_AVAILABLE:
            return np.zeros(len(X))
        
        model.eval()
        X_tensor = torch.FloatTensor(X)
        
        if torch.cuda.is_available():
            X_tensor = X_tensor.cuda()
        
        with torch.no_grad():
            outputs = model(X_tensor)
            predictions = torch.argmax(outputs, dim=1)
        
        return predictions.cpu().numpy()
    
    def evaluate_model(self, model, X_train, X_val, y_train, y_val, model_name):
        """
        –û—Ü–µ–Ω–∏–≤–∞–µ—Ç –º–æ–¥–µ–ª—å —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏ (–ø–æ–¥–¥–µ—Ä–∂–∫–∞ 3 –∫–ª–∞—Å—Å–æ–≤)
        """
        try:
            # –£–±–∏—Ä–∞–µ–º sample_weight –∏–∑ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –µ—Å–ª–∏ –µ—Å—Ç—å
            X_train_clean = X_train.drop('sample_weight', axis=1, errors='ignore')
            X_val_clean = X_val.drop('sample_weight', axis=1, errors='ignore')
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—É—é –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫—É (–±–µ–∑ SMOTE)
            X_train_balanced, y_train_balanced = X_train_clean, y_train
            
            # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å (–±–µ–∑ –≤–µ—Å–æ–≤)
            model.fit(X_train_balanced, y_train_balanced)
            
            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
            y_pred = model.predict(X_val_clean)
            y_pred_proba = model.predict_proba(X_val_clean)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–µ –ø—É—Å—Ç—ã–µ
            if len(y_pred) == 0:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞: –ø—É—Å—Ç—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–ª—è {model_name}")
                return {
                    'accuracy': 0.0,
                    'precision': 0.0,
                    'recall': 0.0,
                    'f1': 0.0,
                    'roc_auc': 0.0
                }, None
            
            # –ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –º–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
            try:
                roc_auc = roc_auc_score(y_val, y_pred_proba, multi_class='ovr', average='weighted')
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ ROC AUC –¥–ª—è {model_name}: {e}")
                roc_auc = 0.0  # Fallback –µ—Å–ª–∏ roc_auc –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç
            
            metrics = {
                'accuracy': accuracy_score(y_val, y_pred),
                'precision': precision_score(y_val, y_pred, average='weighted', zero_division=0),
                'recall': recall_score(y_val, y_pred, average='weighted', zero_division=0),
                'f1': f1_score(y_val, y_pred, average='weighted', zero_division=0),
                'roc_auc': roc_auc
            }
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –≤—Å–µ –º–µ—Ç—Ä–∏–∫–∏ –Ω–µ nan
            for key, value in metrics.items():
                if np.isnan(value):
                    print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞: nan –∑–Ω–∞—á–µ–Ω–∏–µ –¥–ª—è {key} –≤ {model_name}")
                    metrics[key] = 0.0
            
            # –í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            feature_importance = None
            if hasattr(model, 'feature_importances_'):
                feature_importance = model.feature_importances_
            elif hasattr(model, 'coef_'):
                feature_importance = np.abs(model.coef_[0])
            
            return metrics, feature_importance
            
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –≤ evaluate_model –¥–ª—è {model_name}: {e}")
            import traceback
            traceback.print_exc()
            return {
                'accuracy': 0.0,
                'precision': 0.0,
                'recall': 0.0,
                'f1': 0.0,
                'roc_auc': 0.0
            }, None
    
    def time_series_cross_validation(self, X, y, n_splits=8):  # –£–≤–µ–ª–∏—á–µ–Ω–æ —Å 5 –¥–æ 8
        """
        –ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è —Å –≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –æ–∫–Ω–∞–º–∏
        –£–ª—É—á—à–µ–Ω–æ –¥–ª—è –±–æ–ª–µ–µ —Ç—â–∞—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Å —É–≤–µ–ª–∏—á–µ–Ω–Ω–æ–π —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π
        """
        print(f"üîÑ –ó–∞–ø—É—Å–∫ –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–∏ —Å {n_splits} —Ñ–æ–ª–¥–∞–º–∏...")
        
        tscv = TimeSeriesSplit(n_splits=n_splits)
        models_config = self.get_models_for_optuna()
        
        results = {}
        feature_importance = {}
        
        for model_name, config in models_config.items():
            print(f"   –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ {model_name}...")
            
            cv_scores = {
                'accuracy': [], 'precision': [], 'recall': [], 
                'f1': [], 'roc_auc': []
            }
            
            all_feature_importance = []
            best_model = None
            best_score = 0
            
            for fold, (train_idx, val_idx) in enumerate(tscv.split(X)):
                X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
                y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]
                
                # –û—Ç–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ñ–æ–ª–¥–∞
                X_train_selected, selected_features = self.select_features(
                    X_train, y_train, method='kbest', n_features=50
                )
                X_val_selected = X_val[selected_features]
                
                # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
                X_train_scaled = self.scaler.fit_transform(X_train_selected)
                X_val_scaled = self.scaler.transform(X_val_selected)
                
                # –í—ã–±–∏—Ä–∞–µ–º –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –º–µ—Ç–æ–¥ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
                if config['params']:
                    print(f"      –§–æ–ª–¥ {fold+1}: –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è ({model_name})...")
                    
                    # –°–æ–∑–¥–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω–æ–µ –∏–º—è –¥–ª—è –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è
                    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")[:-3]  # –ú–∏–∫—Ä–æ—Å–µ–∫—É–Ω–¥—ã –¥–ª—è —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏
                    study_name = f"{model_name}_{self.timeframe}_fold{fold+1}_{timestamp}"
                    
                    # –í—ã–±–∏—Ä–∞–µ–º –ª—É—á—à–∏–π –∞–ª–≥–æ—Ä–∏—Ç–º –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –º–æ–¥–µ–ª–∏
                    if model_name in ['XGBoost', 'LightGBM', 'CatBoost']:
                        # Hyperband –¥–ª—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≥–æ –±—É—Å—Ç–∏–Ω–≥–∞ (ASHA –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç max_resource)
                        study = optuna.create_study(
                            study_name=study_name,
                            direction='maximize',
                            sampler=optuna.samplers.TPESampler(n_startup_trials=20),
                            pruner=optuna.pruners.HyperbandPruner(
                                min_resource=20,
                                max_resource=200,
                                reduction_factor=3
                            )
                        )
                        n_trials = 50  # –£–≤–µ–ª–∏—á–µ–Ω–æ —Å 10 –¥–æ 50
                        
                    elif model_name in ['RandomForest', 'ExtraTrees']:
                        # BOHB (Bayesian Optimization and HyperBand) –¥–ª—è –∞–Ω—Å–∞–º–±–ª–µ–≤—ã—Ö –º–µ—Ç–æ–¥–æ–≤
                        study = optuna.create_study(
                            study_name=study_name,
                            direction='maximize',
                            sampler=optuna.samplers.TPESampler(n_startup_trials=25),
                            pruner=optuna.pruners.HyperbandPruner(
                                min_resource=10,
                                max_resource=100,
                                reduction_factor=2
                            )
                        )
                        n_trials = 80  # –£–≤–µ–ª–∏—á–µ–Ω–æ —Å 40 –¥–æ 80
                        
                    elif model_name == 'DeepNeuralNetwork':
                        # Hyperband –¥–ª—è –≥–ª—É–±–æ–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
                        study = optuna.create_study(
                            study_name=study_name,
                            direction='maximize',
                            sampler=optuna.samplers.TPESampler(n_startup_trials=10),
                            pruner=optuna.pruners.HyperbandPruner(
                                min_resource=5,
                                max_resource=50,
                                reduction_factor=2
                            )
                        )
                        n_trials = 60  # –£–≤–µ–ª–∏—á–µ–Ω–æ —Å 30 –¥–æ 60
                        
                    elif model_name == 'SVM':
                        # CMA-ES –¥–ª—è SVM
                        study = optuna.create_study(
                            study_name=study_name,
                            direction='maximize',
                            sampler=optuna.samplers.CmaEsSampler(),
                            pruner=optuna.pruners.MedianPruner(
                                n_startup_trials=10,
                                n_warmup_steps=20
                            )
                        )
                        n_trials = 50  # –£–≤–µ–ª–∏—á–µ–Ω–æ —Å 25 –¥–æ 50
                        
                    else:
                        # TPE —Å MedianPruner –¥–ª—è –æ—Å—Ç–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
                        study = optuna.create_study(
                            study_name=study_name,
                            direction='maximize',
                            sampler=optuna.samplers.TPESampler(n_startup_trials=20),
                            pruner=optuna.pruners.MedianPruner(
                                n_startup_trials=10,
                                n_warmup_steps=20
                            )
                        )
                        n_trials = 60  # –£–≤–µ–ª–∏—á–µ–Ω–æ —Å 30 –¥–æ 60
                    
                    # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è
                    try:
                        print(f"        –ó–∞–ø—É—Å–∫ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –±–µ–∑ —Ç–∞–π–º–∞—É—Ç–∞...")
                        
                        study.optimize(
                            lambda trial: self.optuna_objective(
                                trial, X_train_scaled, X_val_scaled, y_train, y_val, model_name
                            ),
                            n_trials=n_trials,
                            show_progress_bar=False
                        )
                    except Exception as e:
                        print(f"      ‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏: {e}")
                        print(f"      –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é –¥–ª—è —ç—Ç–æ–≥–æ —Ñ–æ–ª–¥–∞...")
                        continue
                    
                    # –ü–æ–ª—É—á–∞–µ–º –ª—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏ —Å–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å
                    best_params = study.best_params
                    if model_name == 'DeepNeuralNetwork':
                        model = self.create_deep_learning_model(X_train_scaled.shape[1], best_params)
                    else:
                        model = self.create_model_with_params(model_name, best_params)
                    
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã Optuna
                    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                    study_file = f'model_results/optuna/{model_name}_{self.timeframe}_fold{fold+1}_{timestamp}.pkl'
                    with open(study_file, 'wb') as f:
                        pickle.dump(study, f)
                    
                else:
                    # –ü—Ä–æ—Å—Ç–æ–µ –æ–±—É—á–µ–Ω–∏–µ –±–µ–∑ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
                    if model_name == 'DeepNeuralNetwork':
                        model = self.create_deep_learning_model(X_train_scaled.shape[1], {})
                    else:
                        model = config['model']
                
                # –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ —Å –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–π –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–æ–π
                try:
                    metrics, importance = self.evaluate_model_advanced(
                        model, X_train_scaled, X_val_scaled, y_train, y_val, model_name
                    )
                    
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                    for metric, value in metrics.items():
                        cv_scores[metric].append(value)
                    
                    if importance is not None:
                        all_feature_importance.append(importance)
                    
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å (–∏—Å–ø–æ–ª—å–∑—É–µ–º F1-score)
                    if metrics['f1'] > best_score:
                        best_score = metrics['f1']
                        best_model = model
                        
                except Exception as e:
                    print(f"      ‚ùå –û—à–∏–±–∫–∞ –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–∏ –≤ —Ñ–æ–ª–¥–µ {fold+1}: {e}")
                    print(f"      –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Ñ–æ–ª–¥ {fold+1} –∏ –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º...")
                    continue
            
            # –°—Ä–µ–¥–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            avg_results = {}
            for metric, values in cv_scores.items():
                avg_results[metric] = np.mean(values)
                avg_results[f'{metric}_std'] = np.std(values)
            
            results[model_name] = avg_results
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å
            self.models[model_name] = best_model
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            if all_feature_importance:
                feature_importance[model_name] = np.mean(all_feature_importance, axis=0)
            
            print(f"   ‚úÖ {model_name} –∑–∞–≤–µ—Ä—à–µ–Ω")
            print(f"      F1: {avg_results['f1']:.4f} ¬± {avg_results['f1_std']:.4f}")
            print(f"      ROC-AUC: {avg_results['roc_auc']:.4f} ¬± {avg_results['roc_auc_std']:.4f}")
        
        return results, feature_importance
    
    def evaluate_model_advanced(self, model, X_train, X_val, y_train, y_val, model_name):
        """
        –û—Ü–µ–Ω–∏–≤–∞–µ—Ç –º–æ–¥–µ–ª—å –±–µ–∑ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏ –∫–ª–∞—Å—Å–æ–≤ (–µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ)
        """
        try:
            print(f"      –û–±—É—á–µ–Ω–∏–µ {model_name} –±–µ–∑ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏ (–µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ)...")
            print(f"        –†–∞–∑–º–µ—Ä –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö: {X_train.shape}")
            print(f"        –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤: {np.bincount(y_train)}")
            
            # –û–±—É—á–µ–Ω–∏–µ –±–µ–∑ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏
            if model_name == 'DeepNeuralNetwork':
                self.train_deep_learning_model(model, X_train, X_val, y_train, y_val, {})
                y_pred = self.predict_deep_learning(model, X_val)
                y_pred_proba = None  # PyTorch –Ω–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
            else:
                model.fit(X_train, y_train)
                y_pred = model.predict(X_val)
                y_pred_proba = model.predict_proba(X_val)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–µ –ø—É—Å—Ç—ã–µ
            if len(y_pred) == 0:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞: –ø—É—Å—Ç—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–ª—è {model_name}")
                return {
                    'accuracy': 0.0,
                    'precision': 0.0,
                    'recall': 0.0,
                    'f1': 0.0,
                    'roc_auc': 0.0
                }, None
            
            # –ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –º–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
            try:
                roc_auc = roc_auc_score(y_val, y_pred_proba, multi_class='ovr', average='weighted') if y_pred_proba is not None else 0.0
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ ROC AUC –¥–ª—è {model_name}: {e}")
                roc_auc = 0.0
                
            metrics = {
                'accuracy': accuracy_score(y_val, y_pred),
                'precision': precision_score(y_val, y_pred, average='weighted', zero_division=0),
                'recall': recall_score(y_val, y_pred, average='weighted', zero_division=0),
                'f1': f1_score(y_val, y_pred, average='weighted', zero_division=0),
                'roc_auc': roc_auc
            }
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –≤—Å–µ –º–µ—Ç—Ä–∏–∫–∏ –Ω–µ nan
            for key, value in metrics.items():
                if np.isnan(value):
                    print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞: nan –∑–Ω–∞—á–µ–Ω–∏–µ –¥–ª—è {key} –≤ {model_name}")
                    metrics[key] = 0.0
            
            # –í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            feature_importance = None
            if hasattr(model, 'feature_importances_'):
                feature_importance = model.feature_importances_
            elif hasattr(model, 'coef_'):
                feature_importance = np.abs(model.coef_[0])
            
            return metrics, feature_importance
            
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –≤ evaluate_model_advanced –¥–ª—è {model_name}: {e}")
            import traceback
            traceback.print_exc()
            return {
                'accuracy': 0.0,
                'precision': 0.0,
                'recall': 0.0,
                'f1': 0.0,
                'roc_auc': 0.0
            }, None
    
    def train_final_models(self, X, y):
        """
        –û–±—É—á–∞–µ—Ç —Ñ–∏–Ω–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ –Ω–∞ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö —Å –ª—É—á—à–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ (–±–µ–∑ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏)
        """
        print("üéØ –û–±—É—á–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π...")
        
        # –û—Ç–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        X_selected, selected_features = self.select_features(X, y, method='kbest', n_features=50)
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        X_scaled = self.scaler.fit_transform(X_selected)
        
        # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª–∏ —Å –ª—É—á—à–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ (–±–µ–∑ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏)
        for model_name, model in self.models.items():
            print(f"   –§–∏–Ω–∞–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ {model_name}...")
            model.fit(X_scaled, y)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö
        self.selected_features = selected_features
        
        print(f"‚úÖ –û–±—É—á–µ–Ω–æ {len(self.models)} —Ñ–∏–Ω–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π")
    
    def save_results(self, results, feature_importance):
        """
        –°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—É—á–µ–Ω–∏—è
        """
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        results_file = f'model_results/validation/results_{self.timeframe}_{timestamp}.json'
        with open(results_file, 'w') as f:
            json.dump(results, f, indent=2, default=str)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        if feature_importance:
            importance_file = f'model_results/features/importance_{self.timeframe}_{timestamp}.json'
            importance_data = {}
            for model_name, importance in feature_importance.items():
                importance_data[model_name] = importance.tolist()
            
            with open(importance_file, 'w') as f:
                json.dump(importance_data, f, indent=2)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª–∏
        for model_name, model in self.models.items():
            model_file = f'model_results/models/{model_name}_{self.timeframe}_{timestamp}.pkl'
            with open(model_file, 'wb') as f:
                pickle.dump(model, f)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–∫–∞–ª–µ—Ä –∏ –ø—Ä–∏–∑–Ω–∞–∫–∏
        scaler_file = f'model_results/models/scaler_{self.timeframe}_{timestamp}.pkl'
        with open(scaler_file, 'wb') as f:
            pickle.dump(self.scaler, f)
        
        features_file = f'model_results/features/selected_features_{self.timeframe}_{timestamp}.json'
        with open(features_file, 'w') as f:
            json.dump(self.selected_features, f, indent=2)
        
        print(f"üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã:")
        print(f"   –†–µ–∑—É–ª—å—Ç–∞—Ç—ã: {results_file}")
        print(f"   –ú–æ–¥–µ–ª–∏: model_results/models/")
        print(f"   –ü—Ä–∏–∑–Ω–∞–∫–∏: model_results/features/")
    
    def create_comparison_plot(self, results):
        """
        –°–æ–∑–¥–∞–µ—Ç –≥—Ä–∞—Ñ–∏–∫ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π
        """
        print("üìä –°–æ–∑–¥–∞–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–∞ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è...")
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        metrics = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']
        model_names = list(results.keys())
        
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        axes = axes.flatten()
        
        for i, metric in enumerate(metrics):
            values = [results[model][metric] for model in model_names]
            errors = [results[model][f'{metric}_std'] for model in model_names]
            
            bars = axes[i].bar(model_names, values, yerr=errors, capsize=5)
            axes[i].set_title(f'{metric.upper()}')
            axes[i].set_ylabel('Score')
            axes[i].tick_params(axis='x', rotation=45)
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –Ω–∞ —Å—Ç–æ–ª–±—Ü—ã
            for bar, value in zip(bars, values):
                height = bar.get_height()
                axes[i].text(bar.get_x() + bar.get_width()/2., height + 0.01,
                           f'{value:.3f}', ha='center', va='bottom')
        
        # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–π subplot
        axes[-1].remove()
        
        plt.tight_layout()
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≥—Ä–∞—Ñ–∏–∫
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        plot_file = f'model_results/validation/comparison_{self.timeframe}_{timestamp}.png'
        plt.savefig(plot_file, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"ÔøΩÔøΩ –ì—Ä–∞—Ñ–∏–∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {plot_file}")
    
    def print_results(self, results):
        """
        –í—ã–≤–æ–¥–∏—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –∫—Ä–∞—Å–∏–≤–æ–º —Ñ–æ—Ä–º–∞—Ç–µ (–ø–æ–¥–¥–µ—Ä–∂–∫–∞ 3 –∫–ª–∞—Å—Å–æ–≤)
        """
        print(f"\nüèÜ –†–ï–ó–£–õ–¨–¢–ê–¢–´ –°–†–ê–í–ù–ï–ù–ò–Ø –ú–û–î–ï–õ–ï–ô ({self.timeframe.upper()})")
        print("=" * 80)
        print("–í–∫–ª—é—á–∞–µ—Ç: Optuna —Å BOHB/Hyperband/MedianPruner (–±–µ–∑ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏)")
        print("–¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è: 0 (—à–æ—Ä—Ç), 1 (–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ), 2 (–ª–æ–Ω–≥)")
        print("=" * 80)
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ ROC AUC
        sorted_results = sorted(results.items(), key=lambda x: x[1]['roc_auc'], reverse=True)
        
        print(f"{'–ú–æ–¥–µ–ª—å':<20} {'Accuracy':<10} {'Precision':<10} {'Recall':<10} {'F1':<10} {'ROC AUC':<10}")
        print("-" * 80)
        
        for model_name, metrics in sorted_results:
            print(f"{model_name:<20} "
                  f"{metrics['accuracy']:<10.3f} "
                  f"{metrics['precision']:<10.3f} "
                  f"{metrics['recall']:<10.3f} "
                  f"{metrics['f1']:<10.3f} "
                  f"{metrics['roc_auc']:<10.3f}")
        
        print("-" * 80)
        
        # –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å
        best_model = sorted_results[0]
        print(f"ü•á –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å: {best_model[0]} (ROC AUC: {best_model[1]['roc_auc']:.3f})")
        print(f"ü•à –í—Ç–æ—Ä–∞—è –º–æ–¥–µ–ª—å: {sorted_results[1][0]} (ROC AUC: {sorted_results[1][1]['roc_auc']:.3f})")
        print(f"ü•â –¢—Ä–µ—Ç—å—è –º–æ–¥–µ–ª—å: {sorted_results[2][0]} (ROC AUC: {sorted_results[2][1]['roc_auc']:.3f})")
        
        print(f"\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫–ª–∞—Å—Å–æ–≤:")
        print(f"   -1 (—à–æ—Ä—Ç): {self.short_samples} ({self.short_samples/self.total_samples*100:.1f}%)")
        print(f"    0 (–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ): {self.neutral_samples} ({self.neutral_samples/self.total_samples*100:.1f}%)")
        print(f"   +1 (–ª–æ–Ω–≥): {self.long_samples} ({self.long_samples/self.total_samples*100:.1f}%)")

def main():
    """
    –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è
    """
    print("üöÄ Model Trainer - –°–∏—Å—Ç–µ–º–∞ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π")
    print("=" * 60)
    print("‚ú® –í–∫–ª—é—á–∞–µ—Ç: Optuna —Å –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–º–∏ –∞–ª–≥–æ—Ä–∏—Ç–º–∞–º–∏ (–±–µ–∑ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏)")
    print("üéØ –¶–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –∏ –≥–æ—Ä–∏–∑–æ–Ω—Ç—ã –ù–ï –ò–ó–ú–ï–ù–Ø–Æ–¢–°–Ø!")
    print("=" * 60)
    
    # –°–æ–∑–¥–∞–µ–º —Ç—Ä–µ–Ω–µ—Ä –¥–ª—è 5M
    trainer_5m = ModelTrainer(timeframe='5m')
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    df_5m = trainer_5m.load_data()
    
    # –°–æ–∑–¥–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
    df_5m = trainer_5m.create_features(df_5m)
    
    # –°–æ–∑–¥–∞–µ–º —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é (–ù–ï –ò–ó–ú–ï–ù–Ø–ï–ú!)
    df_5m = trainer_5m.create_target(df_5m)
    
    # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    feature_columns = [col for col in df_5m.columns if col not in ['timestamp', 'datetime', 'target']]
    X_5m = df_5m[feature_columns].fillna(0)
    y_5m = df_5m['target']
    
    print(f"üìä –î–∞–Ω–Ω—ã–µ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω—ã:")
    print(f"   –ü—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(feature_columns)}")
    print(f"   –ü—Ä–∏–º–µ—Ä–æ–≤: {len(X_5m)}")
    print(f"   –ü–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö: {y_5m.sum()} ({y_5m.mean()*100:.1f}%)")
    
    # –í—Ä–µ–º–µ–Ω–Ω–∞—è –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π
    results_5m, importance_5m = trainer_5m.time_series_cross_validation(X_5m, y_5m)
    
    # –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    trainer_5m.print_results(results_5m)
    
    # –°–æ–∑–¥–∞–µ–º –≥—Ä–∞—Ñ–∏–∫
    trainer_5m.create_comparison_plot(results_5m)
    
    # –û–±—É—á–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏
    trainer_5m.train_final_models(X_5m, y_5m)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    trainer_5m.save_results(results_5m, importance_5m)
    
    print(f"\nüéâ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ –¥–ª—è 5M —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞!")
    
    # –ê–Ω–∞–ª–æ–≥–∏—á–Ω–æ –¥–ª—è 15M
    print(f"\n" + "="*60)
    print("üîÑ –û–±—É—á–µ–Ω–∏–µ –¥–ª—è 15M —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞...")
    
    trainer_15m = ModelTrainer(timeframe='15m')
    df_15m = trainer_15m.load_data()
    df_15m = trainer_15m.create_features(df_15m)
    df_15m = trainer_15m.create_target(df_15m)
    
    X_15m = df_15m[feature_columns].fillna(0)
    y_15m = df_15m['target']
    
    results_15m, importance_15m = trainer_15m.time_series_cross_validation(X_15m, y_15m)
    trainer_15m.print_results(results_15m)
    trainer_15m.create_comparison_plot(results_15m)
    trainer_15m.train_final_models(X_15m, y_15m)
    trainer_15m.save_results(results_15m, importance_15m)
    
    print(f"\nüéâ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ –¥–ª—è 15M —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞!")
    print(f"\nüìÅ –í—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –ø–∞–ø–∫–µ model_results/")

if __name__ == "__main__":
    main()